# ğŸ” HackGeek Research Agent

An intelligent research agent with persistent memory, built for the HackGeek Hackathon by GeekRoom.

## ğŸš€ What It Does

HackGeek Research Agent answers research queries using a full RAG (Retrieval-Augmented Generation) pipeline. It remembers past sessions per user, retrieves relevant context from a vector database, and generates detailed answers using a fast LLM.

## ğŸ§  Architecture
```
User Query
    â†“
Router (quick / deep mode detection)
    â†“
Memory Retrieval (Qdrant vector DB)
    â†“
LLM Answer Generation (Groq + Llama 3)
    â†“
Memory Storage (Qdrant)
    â†“
Response (Answer + Memory + Metrics)
```

## ğŸ› ï¸ Tech Stack

| Component | Technology |
|---|---|
| Frontend | Streamlit |
| LLM | Groq (Llama 3.1 8B) |
| Vector DB | Qdrant Cloud |
| Embeddings | Ollama (nomic-embed-text) |
| Memory | Qdrant (user_preferences, research_history, key_facts) |

## âœ¨ Features

- **Fast responses** via Groq API (under 2 seconds)
- **Persistent memory** â€” remembers past research per user
- **Quick & Detailed modes** â€” auto-detected or manually selected
- **Per-user sessions** â€” separate memory for each user ID
- **Metrics tracking** â€” latency and cost per query
- **Debug info** â€” model, context used, Qdrant hits
- **100% free to run** â€” Groq free tier + Qdrant free tier + Ollama local

## ğŸ“¦ Installation

### 1. Clone the repo
```bash
git clone https://github.com/yourname/hackgeek-research-agent
cd hackgeek-research-agent/tech_research_agent
```

### 2. Install dependencies
```bash
pip install -r requirements.txt
```

### 3. Install and start Ollama
Download from [ollama.com](https://ollama.com) then run:
```bash
ollama pull nomic-embed-text
ollama serve
```

### 4. Set up environment variables
Create a `.env` file in the `tech_research_agent` folder:
```
GROQ_API_KEY=your_groq_api_key
QDRANT_URL=your_qdrant_cluster_url
QDRANT_API_KEY=your_qdrant_api_key
```

- Get Groq API key free at [console.groq.com](https://console.groq.com)
- Get Qdrant free cluster at [cloud.qdrant.io](https://cloud.qdrant.io)

### 5. Initialize Qdrant collections
```bash
python memory/qdrant_db.py
```

### 6. Run the app
```bash
streamlit run app.py
```

## ğŸ“ Project Structure
```
tech_research_agent/
â”œâ”€â”€ app.py                  # Streamlit UI
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ agent_controller.py # Main agent orchestrator
â”‚   â”œâ”€â”€ executor.py         # Groq LLM calls
â”‚   â”œâ”€â”€ router.py           # Quick/deep mode detection
â”‚   â””â”€â”€ clarifier.py        # Query clarification
â”œâ”€â”€ memory/
â”‚   â”œâ”€â”€ qdrant_db.py        # Qdrant client + collections
â”‚   â”œâ”€â”€ retrieve.py         # Vector retrieval
â”‚   â””â”€â”€ store.py            # Vector storage
â”œâ”€â”€ retrieval/
â”‚   â””â”€â”€ citation.py         # Citation handling
â”œâ”€â”€ .env                    # API keys (not committed)
â””â”€â”€ requirements.txt
```

## ğŸ¯ How Memory Works

Every query is embedded using `nomic-embed-text` and stored in Qdrant under the user's ID. On the next query, relevant past research, preferences, and key facts are retrieved and injected into the prompt â€” making answers smarter over time.

## ğŸ‘¨â€ğŸ’» Built By

Raj â€” HackGeek Hackathon, GeekRoom 2026

